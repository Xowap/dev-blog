---
title: 0 bullshit tips to stay relevant in the dev industry
published: false
description: Deep down a lot of developers fear becoming irrelevant to this ever-changing industry. How can one get into a safer spot?
cover_image: https://thepracticaldev.s3.amazonaws.com/i/b6ubehysx9jm3duhj72m.jpg
canonical_url: https://github.com/Xowap/dev-blog/blob/master/articles/essays/0-bullshit-tips-to-stay-relevant-in-the-dev-industry.md
---

I have lately been reading a lot of comments and articles from other developers and I am getting the unmistakable feeling that a lot of us are frightened to become irrelevant. To get stuck 10 years on a shitty dusty stack that nobody wants anymore.

The more I was seeing those comments the more I started to realize that I have myself been stuck in the same old stack for almost 10 years. Indeed, my first Django application [was initiated](https://github.com/Xowap/Maiznet/commit/a959a0517d93acb91c237bafa90effb19010f453) in 2011, 8 years ago.

Yet, this is very contradictory to my inner feeling of using the best tool for the job. For me, Django was the best tool for most of my jobs in 2011 and still is the best tool in 2019.

Which begs the question. Am I an old fart too sure of himself? Or am I a visionary genius capable of ever-lasting good decisions?

That is when I started thinking about what it meant to stay up-to-date as a developer.

## How fast does it move?

The premise of the whole "being outdated" idea is that technology is moving so fast that you can't keep up with it. Let's then evaluate the change rate of technology before moving deeper into this essay.

So, maybe we are talking about the platforms. Windows was released in 1985, Linux in 1991 and OS X in 2001 but it's more like a GUI on top of FreeBSD (1993)/Mach (1985). It looks like desktop/server operating systems are more or less 30 years old.

Could it be that we are talking about the web? Which was invented in 1989. It's taken up to about 2010 for HTML 5 to start kicking out. Most of the things we do today come with polyfills up to IE6 (2001). I'm not saying that you could do everything on IE6 but I'm pretty sure you could run a material-ish SPA in IE6.

Talking about SPAs, they appeared on my radar around 2012. The previous major change before that was AJAX in 2004. And before that it would be the apparition of JavaScript itself in 1995.

So yes, there is a zillion of frameworks doing exactly the same thing in different ways out there but SPAs are 8-years-old in my memory.

Based on the dates above, development paradigms seen to be shifting more or less every ten years. I see a lot of resume of people changing jobs every 2 years, it gives them the time for 5 jobs per "paradigm era".

## How much is there to learn?

Since this post is a bit about me, let's see how I became a developer.

To me, software development is as evident as  drinking water. There is no specific purpose to it. I just intrinsically needs it and always have. Up until when I was about 7 when I first started playing with BASIC. Then VisualBasic.

Then PHP. Then I forgot the first two and just knew PHP. Then from age 13 to age 18 I did code only in PHP and was frightened as hell to learn anything else. That C language felt so imposing and frightening. Let's not even dare talking about C++.

Then I joined an engineering school. The first year, seeing that other people with no prior knowledge of programming were tasked to do so, I learned C. I'm still not a master of C today because I don't use nor need it, however at the time my results were outstanding in comparison of the rest of the class. All of that was the result of doing one day of learning C.

Of course, all my friends with prior knowledge of programming did exactly the same as I did. But the dreaded C was vanquished in one small day. *One* day.

Then I went on learning. Maths, physics, electronics, economy, signal theory and so many things about which I had no prior knowledge. It took me five years overall to complete the course, which is the normal time to do it.

Here is what I know now. In my training as an engineer, learning **C is one day within five years**.

> *Note* &mdash; Heck, I wish I remembered what I did those 1825 other days

Now if you consider that paradigms will shift every 10 years, if you can move from one stack to another within that paradigm within a matter of days or even a few weeks then who cares? Hangovers alone are a greater threat to my productivity! People need to **chill the fuck out**.

## What was the question?

As [Deep Thought](https://simple.wikipedia.org/wiki/42_(answer)) and [Elon Musk](https://youtu.be/cIQ36Kt7UVg?t=316) stated, the answer doesn't really matter if you don't really have a question. You can find online a bazillion articles giving you an astounding list of podcasts, blogs and Twitter accounts to follow in order to save you from the eternal damnation of boring stacks.

So, tell me what you want, what you really really want?

If you wanna be an engineer then there is probably two things that matter a lot:

- Be good in your current job
- Be desirable for another job

Those two seem a bit contradictory. Indeed, to stay at your current job you need to specialize in whatever technologies the company you're in uses while on the other hand to be desirable for another job you need to know the hot shiny stuff that your company isn't using.

Does it mean that you need to spend countless hours on personal projects on your free time so you don't end up locked in your soon-to-be-legacy job?

Most companies can't get you on commercial projects with new shiny stuff because the company found its balance and inner ecosystem. Everyone handles well the same tools and introducing something new is slow and time-taking. But most of all introducing something new means introducing [incertainty in the planning](https://dev.to/xowap/the-way-adults-deliver-software-on-time-2bio), which is not necessarily a good idea.

However there is small opportunities to test things out. Internal projects, one-shot short projects, periods of low activity, ... In France and most probably in other countries there is even a dedicated training fund for each employee that you can use to get your employer pay you a training on a specific topic. And then of course there is the possibility to train on your free time.

In short there is opportunities to learn but their span is limited. The amount of time you get depends on many factors but in the end you need to focus your attention. And this make our question much more obvious.

> **What should I focus my learning on?**

## The basics

One of my favorite sayings is "*When all you have is a hammer all problems look like a nail*". We humans are surprisingly unimaginative and all we do is repeat patterns that we've seen somewhere before. However, creativity comes in when instead of repeating the same idea pristine over and over again we can mix ideas together.

While this is not a scientific proof at all, try to bear with me down this reasoning that I'm actually stealing from [another article of mine](https://dev.to/xowap/8-tips-from-john-wick-for-10x-developers-366p). Suppose that one new idea comes from the [combination](https://en.wikipedia.org/wiki/Combination) of two old ideas. In that case the number of ideas you can produce depends on how many ideas you already have.

- 2 ideas can give 1 more idea
- 3 ideas can give 3 more ideas
- 10 ideas can give 45 more ideas
- 100 ideas can give 4950 more ideas

At this point it becomes obvious that the number of new ideas you can have increases deeply with the amount of things you already know. And the further apart those things are the more you can breathe fresh air into your problems.

For this reason I recommend to get a wide scientific culture. The boring math-y stuff. What are Markov chains? How does electricity behaves? How do you make a spring resonate? How do you quantify information? All that kind of things that have nothing to do with programming. I used equations from all things mentioned above into algorithms at some point in my life.

Now while that's what studying is for, if you skipped that part you can always catch up later. It's not going to be easy and it's not going to be quick, but there is books on most matters, online courses, etc. Unfortunately I don't have any specific resources to point at but nowadays most universities start putting their courses online for free so you should be able to find fine material.

Then comes in the more computer-science-y stuff. There is really two books that stand out within all I've read and both of them helped me repeatedly in my career.

- [Modern Operating Systems](https://en.wikipedia.org/wiki/Modern_Operating_Systems) by Andrew Tanenbaum &mdash; That book was foundational to writing Linux and the recent editions talk about many, may topics that cover most of the theoretical background you'll ever need to have regarding the way computers work.
- [The Algorithm Design Manual](http://www.algorist.com/) by Steven Skiena &mdash; A big fat book that helps you come up with algorithms. There is a very big part of it dedicated to explaining solutions to most common algorithms. This is my go-to book when I have a new algorithm to write, usually I just need to twist one of the provided solutions.

If you master the content of those two books then you'll have in your hands tools that help you understand anything that is likely to be thrown at you over the course of your career. 

